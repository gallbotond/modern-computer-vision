{"cells":[{"cell_type":"markdown","metadata":{"id":"omWff1IYp_y2"},"source":["![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n","# **YOLOv3 in using cv2.dnn.readNetFrom()**\n","\n","####**In this lesson we'll learn how to load a pre-trained YOLOV3 Model and use OpenCV to run inferences over a few images**\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10887,"status":"ok","timestamp":1637439998595,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"4BlZva9GdJVQ","outputId":"015d17cc-be6e-4645-a20d-cbbbf1d0a754"},"outputs":[{"name":"stderr","output_type":"stream","text":["--2024-03-13 11:25:47--  https://drive.google.com/uc?id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4\n","Resolving drive.google.com (drive.google.com)... 142.250.201.206\n","Connecting to drive.google.com (drive.google.com)|142.250.201.206|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4 [following]\n","--2024-03-13 11:25:47--  https://drive.usercontent.google.com/download?id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.180.193\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.180.193|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2368 (2.3K) [text/html]\n","Saving to: 'uc@id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4'\n","\n","     0K ..                                                    100% 4.56M=0s\n","\n","2024-03-13 11:25:48 (4.56 MB/s) - 'uc@id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4' saved [2368/2368]\n","\n","'unzip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["# import the necessary packages\n","import numpy as np\n","import time\n","import cv2\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","from matplotlib import pyplot as plt \n","\n","# Define our imshow function \n","def imshow(title = \"Image\", image = None, size = 10):\n","    w, h = image.shape[0], image.shape[1]\n","    aspect_ratio = w/h\n","    plt.figure(figsize=(size * aspect_ratio,size))\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","    plt.title(title)\n","    plt.show()\n","\n","# Download and unzip our images and YOLO files\n","!wget https://drive.google.com/uc?id=1rEV9xrrctxPwFPKvF3EC0sSNyYAZSEP4\n","!unzip -qq YOLO.zip"]},{"cell_type":"markdown","metadata":{"id":"qhQD1v84dqxu"},"source":["## **YOLO Object Detection**\n","\n","![](https://opencv-tutorial.readthedocs.io/en/latest/_images/yolo1_net.png)\n","\n","**Steps Invovled**\n","\n","1. Use Pretrained YOLOV3 weights (237MB)- https://pjreddie.com/media/files/yolov3.weights\n","2. Create our blob object which is our loaded model\n","3. Set the backend that runs the model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1637440008361,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"Lb-FtCJRdOG8","outputId":"0ab71ca7-a5e8-4de6-da53-207202be59c9"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'YOLO/yolo/coco.names'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the COCO class labels our YOLO model was trained on\u001b[39;00m\n\u001b[0;32m      2\u001b[0m labelsPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLO/yolo/coco.names\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m LABELS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabelsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# We now need to initialize a list of colors to represent each possible class label\u001b[39;00m\n\u001b[0;32m      6\u001b[0m COLORS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(LABELS), \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'YOLO/yolo/coco.names'"]}],"source":["# Load the COCO class labels our YOLO model was trained on\n","labelsPath = \"YOLO/yolo/coco.names\"\n","LABELS = open(labelsPath).read().strip().split(\"\\n\")\n"," \n","# We now need to initialize a list of colors to represent each possible class label\n","COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n","\n","print(\"Loading YOLO weights...\") \n","\n","weights_path = \"YOLO/yolo/yolov3.weights\" \n","cfg_path = \"YOLO/yolo/yolov3.cfg\"\n","\n","# Create our blob object\n","net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n","\n","# Set our backend\n","net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n","# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n","\n","print(\"Our YOLO Layers\")\n","ln = net.getLayerNames()\n","\n","# There are 254 Layers\n","print(len(ln), ln)"]},{"cell_type":"markdown","metadata":{"id":"S8cfxbOwejXq"},"source":["The input to the network is a called blob object. \n","\n","The function ```cv.dnn.blobFromImage(img, scale, size, mean)``` transforms the image into a blob:\n","\n","```blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)```\n","\n","**It has the following parameters:**\n","\n","1. the image to transform\n","2. the scale factor (1/255 to scale the pixel values to [0..1])\n","3. the size, here a 416x416 square image\n","4. the mean value (default=0)\n","5. the option swapBR=True (since OpenCV uses BGR)\n","\n","**Note** A blob is a 4D numpy array object (images, channels, width, height). The image below shows the red channel of the blob. You notice the brightness of the red jacket in the background.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1cGrUGvCSAVWdU6D7iP90_ns8qb0poqmn"},"executionInfo":{"elapsed":22348,"status":"ok","timestamp":1637440325397,"user":{"displayName":"Rajeev Ratan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtO-hUyDqrPmYR3HGcaXMtwRIq1ObsdPjhiGDSWSw=s64","userId":"08597265227091462140"},"user_tz":0},"id":"PtC5_CRLeVV8","outputId":"95c4434f-1763-4b0e-d10c-2ddebbb68001"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Starting Detections...\")\n","# Get images located in ./images folder    \n","mypath = \"YOLO/images/\"\n","file_names = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","\n","# Loop through images run them through our classifer\n","for file in file_names:\n","    # load our input image and grab its spatial dimensions\n","    image = cv2.imread(mypath+file)\n","    (H, W) = image.shape[:2]\n"," \n","    # we want only the *output* layer names that we need from YOLO\n","    ln = net.getLayerNames()\n","    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","    # Now we contruct our blob from our input images\n","    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n","    # We set our input to our image blob\n","    net.setInput(blob)\n","    # Then we run a forward pass through the network\n","    layerOutputs = net.forward(ln)\n","\n","    # we initialize our lists for our detected bounding boxes, confidences, and classes\n","    boxes = []\n","    confidences = []\n","    IDs = []\n","\n","    # Loop over each of the layer outputs\n","    for output in layerOutputs:\n","\n","        # Loop over each detection\n","        for detection in output:\n","            # Obtain class ID and probality of detection\n","            scores = detection[5:]\n","            classID = np.argmax(scores)\n","            confidence = scores[classID]\n","\n","            # We keep only the most probably predictions\n","            if confidence > 0.75:\n","                # We scale the bounding box coordinates relative to the image\n","                # Note: YOLO actually returns the center (x, y) of the bounding\n","                # box followed by the width and height of the box\n","                box = detection[0:4] * np.array([W, H, W, H])\n","                (centerX, centerY, width, height) = box.astype(\"int\")\n","\n","                # Get the the top and left corner of the bounding box\n","                # Remember we alredy have the width and height\n","                x = int(centerX - (width / 2))\n","                y = int(centerY - (height / 2))\n","\n","                # Append our list of bounding box coordinates, confidences and class IDs\n","                boxes.append([x, y, int(width), int(height)])\n","                confidences.append(float(confidence))\n","                IDs.append(classID)\n","\n","    # Now we apply non-maxima suppression to reduce overlapping bounding boxes\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n","\n","    # We proceed once a detection has been found\n","    if len(idxs) > 0:\n","        # iterate over the indexes we are keeping\n","        for i in idxs.flatten():\n","            # Get the bounding box coordinates\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","\n","            # Draw our bounding boxes and put our class label on the image\n","            color = [int(c) for c in COLORS[IDs[i]]]\n","            cv2.rectangle(image, (x, y), (x + w, y + h), color, 3)\n","            text = \"{}: {:.4f}\".format(LABELS[IDs[i]], confidences[i])\n","            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","\n","    # show the output image\n","    imshow(\"YOLO Detections\", image, size = 12)"]},{"cell_type":"markdown","metadata":{"id":"AZjRzsrLotwA"},"source":["## **NOTE:** **How to Perform non maximum suppression given boxes and corresponding scores.**\n","\n","```indices\t=\tcv.dnn.NMSBoxes(\tbboxes, scores, score_threshold, nms_threshold[, eta[, top_k]]```\n","\n","\n","\n","**Parameters**\n","- bboxes\ta set of bounding boxes to apply NMS.\n","- scores\ta set of corresponding confidences.\n","- score_threshold\ta threshold used to filter boxes by score.\n","- nms_threshold\ta threshold used in non maximum suppression.\n","indices\tthe kept indices of bboxes after NMS.\n","- eta\ta coefficient in adaptive threshold formula: nms_thresholdi+1=eta⋅nms_thresholdi.\n","- top_k\tif >0, keep at most top_k picked indices.\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCwVpQLs3aoUhTp8XVEydk","collapsed_sections":[],"name":"33. YOLOv3 in OpenCV.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
